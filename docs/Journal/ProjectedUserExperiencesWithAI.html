<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Projected User Experiences via Generative AI </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Projected User Experiences via Generative AI ">
      
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../styles/docfx.min.css">
      <link rel="stylesheet" href="../styles/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      
      

      <script>
        MathJax = {
          options: {
            processHtmlClass: ['tex2jax_process', 'math']
          }
        };
      </script>

      <script type="text/javascript" src="../styles/docfx.min.js"></script>
      <script type="text/javascript" src="../styles/main.js"></script>
  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="png" src="../logo.png" alt="">
            
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" style="margin-top: -.65em; margin-left: -.8em" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="projected-user-experiences-via-generative-ai">Projected User Experiences via Generative AI</h1>

<p><img src="../Images/ProjectedUserExperiencesWithAI/image1.png" alt=""></p>
<div style="font-size: x-small;color: #333;font-weight:bolder;padding-bottom:12px;margin-top: -16px;">Image: Stable Diffusion - bright background, digital art, masterful,[blue (theme):gold (theme):0.25], conceptual projection of sound to thought, abstraction, face, ear, mind</div>  
<p>This site is about my personal exploration of all sorts of applied AI. I am not a data scientist or mathematician, and I have never been a computer scientist. I design and build systems that bring all sorts of technology and people together. I take joy in learning new things and applying my knowledge to help others.<br><br>
If you look through what I’ve written on this site so far, you’ll see speech recognition (text from speech), image generation (images from text), and text generation (text from text) topics all mixed together with ways of using them in the cloud and locally. A common subtext is the topic of user experience (UX) where I am the user, but I am really interested in learning from these experiences to make new user experiences for other people as part of my art.<br></p>
<h2 id="interesting-perspectives-from-respectable-experts">Interesting Perspectives from Respectable Experts</h2>
<p>I recently saw a post from <a href="https://www.linkedin.com/in/resing/">Tom Resing on LinkedIn</a> titled <strong>Prompt-Driven AI UX Hurts Usability</strong>:</p>
<p><img src="../Images/ProjectedUserExperiencesWithAI/image2.png" alt=""></p>
<p><a href="https://en.wikipedia.org/wiki/Jakob_Nielsen_%28usability_consultant%29">Jakob Nielsen</a> is an undeniable authority on usability and so I eagerly read <a href="https://www.linkedin.com/pulse/prompt-driven-ai-ux-hurts-usability-jakob-nielsen/">the article</a> Tom shared and you should too! I learned a few things and it otherwise re-enforced one thing I already believed text only experiences are <strong>not good fit for a significant percentage of people</strong>. <br><br>
In this respect, text only experiences are similar to most of the software ever produced. In fact, it is similar in this respect to almost every experience (text only or otherwise) ever conceived. Software is hard for you to use because you aren’t a strong reader and writer? Well,</p>
<ul>
<li>Some people can’t read the menu at the local restaurant because they can’t see</li>
<li>Some people can’t read the menu at the local restaurant because they can’t read</li>
<li>Some people can’t learn from lectures because they don’t hear</li>
<li>Some people can’t learn from lectures because they can’t understand what is being said</li>
<li>Some people have mobility issues and can’t participate in activities in locations with lack of access</li>
<li>Etc…</li>
</ul>
<p>None of these are a good things and good people do what they can to help others. I try to be a good person.</p>
<h2 id="dreaming-of-a-better-future">Dreaming of a Better Future</h2>
<p>I think the world would be a better place if media and experiences adapted to people instead of requiring people to do things that they as individuals can’t do for whatever reason. We do our best today. For example, we have braille to help the blind “see” words on a page and RTT/TTY to help the deaf “talk” on the phone. However, what we have today barely scratches the surface when it comes to all the accessibility barriers that exist today. <br></p>
<blockquote>
<p>half of the population in rich countries like the United States and Germany are classified as low-literacy users<br>
This doesn’t mean that half of the population is unintelligent, it means they aren’t good at reading and writing. It should be expected that they won’t find AI based chat to be very helpful for doing complicated things.</p>
</blockquote>
<h1 id="styles-of-learning-and-thinking">Styles of Learning and Thinking</h1>
<p>When I was a boy, I often wondered what it is like to be different people. At some point I came to understand that individuals can be very different from each other and that our basic perception varies considerably. We don’t all think or perceive the same way.</p>
<h2 id="learning-styles-and-the-vark-model">Learning Styles and the VARK Model</h2>
<p>The <a href="https://pubmed.ncbi.nlm.nih.gov/24823519/">VARK model</a> is a learning style theory that categorizes individuals into four main learning styles: Visual, Aural, Reading/Writing, and Kinesthetic. The theory resonates with me. Like many people, my learning styles are Aural and Reading/Writing and according to all the testing they gave me as a child, I’m very good at each. Where visual and kinesthetic learning is concerned, I don’t win any prizes. In fact, I am terrible at remembering people’s faces.</p>
<h2 id="aphantasia-and-anauralia">Aphantasia and Anauralia</h2>
<p>Aphantasia and anauralia describe modes of thought or rather, the lack thereof. Aphantasia, or mental blindness, is the lack of a <em>mind’s eye</em>. I can relate to the concept of aphantasia because generally, the only time I ever <em>see</em> things in my mind is when I dream. Conversely, anauralia is the lack of an inner-dialog. I can’t imagine what that would be like. I don’t only have an inner-dialog, I have an inner-radio station.</p>
<h2 id="ai-for-the-betterment-of-everyone">AI for the Betterment of Everyone</h2>
<p>Multi-modal AI has the potential to make it possible for system builders to tailor experiences to our individual learning and thinking styles with devices appropriate to the user’s physical abilities. Today, the process of UX design is almost always focused on specific devices and form-factors. We make phone apps, web apps, desktop apps and so on. Accessibility concerns are almost always some sort of bolt-on and are usually an afterthought that many people skip if they can get away with it. <br><br>
Making big generalizations about software is usually foolish and someone will read this and say ‘<em>well, actually</em>’, but most modern software is intentionally split between a user interface and back-end services. This is true even if the <em>back-end services</em> are one the same device instead of on the internet and the division between the two layers is purely logical. We do this because:</p>
<ul>
<li>It makes our software easier to build</li>
<li>It makes our software easier to maintain</li>
<li>It makes it easier to reuse components and connect systems to each other</li>
<li>It makes it easier to build different user experiences for the same system</li>
</ul>
<p>In fact, big complicated systems often have more than one way to deliver the same experience in different modalities. What I am envisioning here is, conceptually, more of the same and a variation of an old theme, but now we can think about describing the experience in terms of structured prompts and let specialized generative AI create a UX tailored to the individual. <br><br>
This could translate to:</p>
<ul>
<li>Gestures for the kinesthetic</li>
<li>Speech for the aural</li>
<li>Structured experiences for the visual</li>
<li>And so on..</li>
</ul>
<p>We could project a directed conversational, step by step experienced based on spoken questions and answers for one set of users, a traditional form-based UI for others, and other new modalities for people like Jakob Nielsen to invent. <strong>Don’t be confused, the AI is not creative.</strong></p>
<h2 id="personal-ai-assistants-teachers-and-counselors">Personal AI Assistants, Teachers, and Counselors</h2>
<p>Among the oldest and deepest of human fantasies is the idea of an personal counselor that helps the protagonist in ways that the people around them cannot. Whether as a guardian angel, a fairy godmother, or a wise and animated cricket, we love the idea! A recent favorite of mine is the character Dross from <a href="https://www.willwight.com/">Will White’s recently concluded series, Cradle</a>. <br><br>
It’s fun summer reading if you happen to be that kind of person who thinks that way.</p>
<p><img src="../Images/ProjectedUserExperiencesWithAI/image3.png" alt=""></p>
<div style="font-size: x-small;color: #333;font-weight:bolder;padding-bottom:12px;margin-top: -16px;">Image: Stable Diffusion – Lindon and Dross from Will White’s Cradle series, multiple prompts, checkpoints, and LORAs</div>  
<h2 id="the-friend-we-need">The Friend we Need</h2>
<p>What we have today is very far from an assistant who can stop time so we can work through how to survive a fantasy story’s combat scenarios to say the least, and it is a mistake to anthropomorphize it too much (it is a handy analogy at best). What I am talking about are fancy and specialized AI code generators that work against descriptions of experiences - descriptions designed for interpretation so that the generative AI can project the UX in the appropriate modality for the user.<br><br>
This vision is becoming attainable now and I am incredibly excited and grateful to be here to see it.<br>
--Doug Ware, July 1, 2023</p>
</article>

        <div class="contribution d-print-none">
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>
        
      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>


    <footer class="border-top">
      <div class="container-xxl">
        <div class="flex-fill">
          <span>Follow this project on github <a href="https://github.com/douglasware/ElumenotionSite">Elumenotion Site and Samples</a></span>
        </div>
      </div>
    </footer>
  </body>
</html>