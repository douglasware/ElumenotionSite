{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca15b78",
   "metadata": {},
   "source": [
    "# ONNX Runtime GenAI Interaction Notebook\n",
    "\n",
    "This notebook is configured to demonstrate the use of an ONNX model for generating responses to predefined inputs using the ONNX Runtime GenAI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bf1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import onnxruntime_genai as og\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7566c79",
   "metadata": {},
   "source": [
    "## Load and configure the model, note that these files are not in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da155ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknown provider type: dml",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|user|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m <|end|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<|assistant|>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m og\u001b[38;5;241m.\u001b[39mTokenizer(model)\n\u001b[0;32m     17\u001b[0m tokenizer_stream \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mcreate_stream()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unknown provider type: dml"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "model_path = f'./directml/Phi-3-medium-4k-instruct'  # In tests I used medium-4k and medium-128k\n",
    "# Define search options\n",
    "search_options = {\n",
    "    'do_sample': True,\n",
    "    'max_length': 2048,\n",
    "    'top_p': 0.9,\n",
    "    'top_k': 50,\n",
    "    'temperature': 0.5,\n",
    "    'repetition_penalty': 1.2\n",
    "}\n",
    "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = og.Model(model_path)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e651f8",
   "metadata": {},
   "source": [
    "## Do a test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e2c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: The given problem is to solve an algebraic equation where we have \"x^2 /3 = \u001b[1m3\". Let's start with that step-by-step solution now!\n",
      "\n",
      "Step 1 - Write down the original equation, which has already been done in your question as follows :  \n",
      "`(X ^ 2) / 3 = 3`.   \n",
      "\n",
      "Now let’s move on to find 'x'. Here are the steps involved:\n",
      "\n",
      "Step 2 – Multiply both sides of the equation by `3`, so you can isolate `x²`:    \n",
      "```     \n",
      "((X ^ 2)/3)*3= 3*3       \n",
      "=> X^2 = 9      \n",
      "```        \n",
      "\n",
      "This simplifies our expression greatly because multiplying `(X ^ 2)`/3 by `3` just leaves us with `X ^ \u001b[0;3mx2)`. Now it looks like a much simpler quadratic equation (one variable).\n",
      "\n",
      "Next up...\n",
      "\n",
      "Step 3– Take square root of each side of the new simplified equation (`X^2 = 9`) , remembering when taking roots there will be two solutions (+ or -):         \n",
      "```          \n",
      "√(X^2)=±√9           \n",
      "=> X = +/- 3            \n",
      "```                             \n",
      "Therefore, after following these steps carefully, we see that the value of ‘x’ could either be positive `+3` OR negative `-3`. These would satisfy the initial condition set out at the beginning i.e., \"(x^2)/3\" being equal to `3.`\n",
      "Input tokens: 34, Output tokens: 344\n",
      "Total tokens: 378, Time taken: 4.39 seconds\n",
      "Tokens per second: 86.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predefined messages\n",
    "messages = [\n",
    "    'Solve this equation for x: x squared divided by three equals three by first writing the equation and then solving for x.'\n",
    "]\n",
    "for message in messages:\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Prepare the prompt and encode it to tokens\n",
    "    prompt = f'{chat_template.format(input=message)}'\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    input_token_count = len(input_tokens)\n",
    "\n",
    "    # Initialize generator parameters and the generator\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(**search_options)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "    \n",
    "    # Initialize output token count\n",
    "    output_token_count = 0\n",
    "\n",
    "    print('\\nOutput: ', end='', flush=True)\n",
    "    output_message = ''\n",
    "\n",
    "    # Generate response\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        output_token_count += 1  # Increment output token count\n",
    "        next_output = tokenizer_stream.decode(new_token)\n",
    "        print(next_output, end='', flush=True)\n",
    "        output_message += next_output\n",
    "\n",
    "    del generator  # Free up resources\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "    # Calculate tokens per second\n",
    "    total_time = end_time - start_time\n",
    "    total_tokens = input_token_count + output_token_count\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "    # Print token counts and performance\n",
    "    print(f\"\\nInput tokens: {input_token_count}, Output tokens: {output_token_count}\")\n",
    "    print(f\"Total tokens: {total_tokens}, Time taken: {total_time:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f77fe",
   "metadata": {},
   "source": [
    "## Functions to get the speakers to identify and text near where they appear for possible identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94398587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPEAKER_00', 'SPEAKER_01']\n",
      "[\"[SPEAKER_00] : [(00:00:00.000, 00:00:23.660)] :  All right, everybody. Welcome, welcome, welcome, welcome. First breakout of Build. I'm Barnum Bora, and I lead the developer advocacy team for Microsoft 365 and Copilot Platform.\\n\", '[SPEAKER_00] : [(00:00:23.660, 00:00:24.340)] :  advocacy team for Microsoft 365 and Copilot platform.\\n', '[SPEAKER_00] : [(00:00:27.920, 00:00:28.120)] :  But without further ado, before I come back and do more things,\\n', \"[SPEAKER_00] : [(00:00:31.980, 00:00:36.160)] :  I'm going to hand off to my good friend Jeremy Thake, and he's going to walk you through the first half of this session. Thanks, Barno. I appreciate it. And thank you for coming to Build.\\n\", \"[SPEAKER_01] : [(00:00:37.960, 00:00:40.280)] :  So I'm Jeremy Thake. I'm a\\n\", '[SPEAKER_01] : [(00:00:40.280, 00:00:44.100)] :  principal program manager in the Copilot developer experience team. And a slight\\n']\n",
      "[\"[SPEAKER_00] : [(00:00:00.000, 00:00:23.660)] :  All right, everybody. Welcome, welcome, welcome, welcome. First breakout of Build. I'm Barnum Bora, and I lead the developer advocacy team for Microsoft 365 and Copilot Platform.\\n\", '[SPEAKER_00] : [(00:00:23.660, 00:00:24.340)] :  advocacy team for Microsoft 365 and Copilot platform.\\n', '[SPEAKER_00] : [(00:00:27.920, 00:00:28.120)] :  But without further ado, before I come back and do more things,\\n', \"[SPEAKER_00] : [(00:00:31.980, 00:00:36.160)] :  I'm going to hand off to my good friend Jeremy Thake, and he's going to walk you through the first half of this session. Thanks, Barno. I appreciate it. And thank you for coming to Build.\\n\", \"[SPEAKER_01] : [(00:00:37.960, 00:00:40.280)] :  So I'm Jeremy Thake. I'm a\\n\", '[SPEAKER_01] : [(00:00:40.280, 00:00:44.100)] :  principal program manager in the Copilot developer experience team. And a slight\\n', '[SPEAKER_01] : [(00:00:44.100, 00:00:46.340)] :  plug, you may have heard my voice before.\\n', \"[SPEAKER_01] : [(00:00:46.340, 00:00:49.940)] :  I've run a developer podcast on Microsoft 365 for the last ten years.\\n\", \"[SPEAKER_01] : [(00:00:49.940, 00:00:52.060)] :  And we've just rebooted the show this week.\\n\", \"[SPEAKER_01] : [(00:00:52.060, 00:00:55.700)] :  So if you haven't checked it out, please go on our podcasting apps of choice and download\\n\"]\n",
      "SPEAKER_00:\n",
      "[SPEAKER_00] : [(00:00:00.000, 00:00:23.660)] :  All right, everybody. Welcome, welcome, welcome, welcome. First breakout of Build. I'm Barnum Bora, and I lead the developer advocacy team for Microsoft 365 and Copilot Platform.\n",
      "[SPEAKER_00] : [(00:00:23.660, 00:00:24.340)] :  advocacy team for Microsoft 365 and Copilot platform.\n",
      "[SPEAKER_00] : [(00:00:27.920, 00:00:28.120)] :  But without further ado, before I come back and do more things,\n",
      "[SPEAKER_00] : [(00:00:31.980, 00:00:36.160)] :  I'm going to hand off to my good friend Jeremy Thake, and he's going to walk you through the first half of this session. Thanks, Barno. I appreciate it. And thank you for coming to Build.\n",
      "[SPEAKER_01] : [(00:00:37.960, 00:00:40.280)] :  So I'm Jeremy Thake. I'm a\n",
      "[SPEAKER_01] : [(00:00:40.280, 00:00:44.100)] :  principal program manager in the Copilot developer experience team. And a slight\n",
      "\n",
      "\n",
      "SPEAKER_01:\n",
      "[SPEAKER_00] : [(00:00:00.000, 00:00:23.660)] :  All right, everybody. Welcome, welcome, welcome, welcome. First breakout of Build. I'm Barnum Bora, and I lead the developer advocacy team for Microsoft 365 and Copilot Platform.\n",
      "[SPEAKER_00] : [(00:00:23.660, 00:00:24.340)] :  advocacy team for Microsoft 365 and Copilot platform.\n",
      "[SPEAKER_00] : [(00:00:27.920, 00:00:28.120)] :  But without further ado, before I come back and do more things,\n",
      "[SPEAKER_00] : [(00:00:31.980, 00:00:36.160)] :  I'm going to hand off to my good friend Jeremy Thake, and he's going to walk you through the first half of this session. Thanks, Barno. I appreciate it. And thank you for coming to Build.\n",
      "[SPEAKER_01] : [(00:00:37.960, 00:00:40.280)] :  So I'm Jeremy Thake. I'm a\n",
      "[SPEAKER_01] : [(00:00:40.280, 00:00:44.100)] :  principal program manager in the Copilot developer experience team. And a slight\n",
      "[SPEAKER_01] : [(00:00:44.100, 00:00:46.340)] :  plug, you may have heard my voice before.\n",
      "[SPEAKER_01] : [(00:00:46.340, 00:00:49.940)] :  I've run a developer podcast on Microsoft 365 for the last ten years.\n",
      "[SPEAKER_01] : [(00:00:49.940, 00:00:52.060)] :  And we've just rebooted the show this week.\n",
      "[SPEAKER_01] : [(00:00:52.060, 00:00:55.700)] :  So if you haven't checked it out, please go on our podcasting apps of choice and download\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def list_unique_speakers(file_path):\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find all unique speaker identifiers\n",
    "    speakers = re.findall(r\"\\[SPEAKER_(\\d{2})\\]\", content)\n",
    "    unique_speakers = sorted(set(speakers))\n",
    "    \n",
    "    # Format speakers in the form \"SPEAKER_XX\"\n",
    "    formatted_speakers = [f\"SPEAKER_{speaker}\" for speaker in unique_speakers]\n",
    "    return formatted_speakers\n",
    "\n",
    "\n",
    "print(list_unique_speakers('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt'))\n",
    "\n",
    "def get_context_around_speaker(file_path, speaker):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Find the first occurrence of the speaker\n",
    "    for i, line in enumerate(lines):\n",
    "        if speaker in line:\n",
    "            start_index = max(0, i - 5)  # Ensure start index is within bounds\n",
    "            end_index = min(len(lines), i + 6)  # Ensure end index is within bounds (i+6 for inclusive slicing)\n",
    "            return lines[start_index:end_index]\n",
    "    return []  # Return an empty list if the speaker does not appear\n",
    "\n",
    "print(get_context_around_speaker('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt', list_unique_speakers('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt')[0]))\n",
    "print(get_context_around_speaker('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt', list_unique_speakers('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt')[1]))\n",
    "\n",
    "def collect_speaker_contexts(file_path):\n",
    "    # First, extract unique speakers from the file\n",
    "    unique_speakers = list_unique_speakers(file_path)\n",
    "    \n",
    "    # Now, use the second function to get context for each speaker\n",
    "    speaker_contexts = {}\n",
    "    for speaker in unique_speakers:\n",
    "        # Using the function to fetch context around each speaker's first mention\n",
    "        context = get_context_around_speaker(file_path, speaker)\n",
    "        speaker_contexts[speaker] = context\n",
    "    \n",
    "    return speaker_contexts\n",
    "\n",
    "# Example usage:\n",
    "file_path = './Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt'\n",
    "speaker_context_dict = collect_speaker_contexts(file_path)\n",
    "for speaker, context in speaker_context_dict.items():\n",
    "    print(f\"{speaker}:\")\n",
    "    for line in context:\n",
    "        print(line.strip())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229f5c5",
   "metadata": {},
   "source": [
    "## Simple generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82751bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm just a computer program so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def process_message(message, search_options):\n",
    "    # Prepare the prompt and encode it to tokens\n",
    "    prompt = chat_template.format(input=message)\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "    # Initialize generator parameters and the generator\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(**search_options)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "\n",
    "    # Initialize output message\n",
    "    output_message = ''\n",
    "\n",
    "    # Generate response\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        next_output = tokenizer.decode(new_token)\n",
    "        output_message += next_output\n",
    "\n",
    "    # Free up resources\n",
    "    del generator\n",
    "\n",
    "    return output_message\n",
    "\n",
    "# Example usage:\n",
    "messages = [\"Hello, how are you?\"]\n",
    "for message in messages:\n",
    "    output = process_message(message, search_options)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882dd4c7",
   "metadata": {},
   "source": [
    "## Use the generation function with a prompt to identify each of the speakers and output their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69455a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_00:  Barnum Bora\n",
      "SPEAKER_01:  Jeremy Thake\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt_responses(file_path):\n",
    "    \n",
    "    search_options = {\n",
    "        'do_sample': True,\n",
    "        'max_length': 3578,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 5,\n",
    "        'temperature': 0.5,\n",
    "        'repetition_penalty': 1.2\n",
    "    }\n",
    "\n",
    "    # First, gather the context for each speaker using the previously defined function\n",
    "    speaker_contexts = collect_speaker_contexts(file_path)\n",
    "    \n",
    "    # Iterate through each speaker and their associated context\n",
    "    responses = {}\n",
    "    for speaker, context in speaker_contexts.items():\n",
    "        # Join the context lines into a single string\n",
    "        context_text = ''.join(context).strip()\n",
    "        \n",
    "        # Create the prompt with the current speaker and their context\n",
    "        prompt = f\"\"\"\n",
    "        The following text is a piece of a transcript. Examine the text and reply with ONLY the full name matching the exact spelling of the person listed as {speaker} and nothing else.\n",
    "        ----------------------------------------\n",
    "        {context_text}\"\"\"\n",
    "        \n",
    "        # Call the existing process_message function with the formatted prompt\n",
    "        response = process_message(prompt, search_options)\n",
    "        \n",
    "        # Store the response for this speaker\n",
    "        responses[speaker] = response\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# This function can be used as follows:\n",
    "response_dict = generate_prompt_responses(file_path)\n",
    "for speaker, response in response_dict.items():\n",
    "    print(f\"{speaker}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c722b1",
   "metadata": {},
   "source": [
    "## Create the final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48cedbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as: ./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.FINAL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def update_speakers_with_real_names(file_path):\n",
    "    # Assume responses are in the form of speaker names we want to replace in the original file\n",
    "    response_dict = generate_prompt_responses(file_path)\n",
    "    \n",
    "    # Read the original file content\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "    \n",
    "    # Replace speaker labels with actual names based on response_dict\n",
    "    updated_content = []\n",
    "    for line in content:\n",
    "        for speaker, real_name in response_dict.items():\n",
    "            if speaker in line:\n",
    "                # Replace the first occurrence of speaker in the line with the real name\n",
    "                line = line.replace(speaker, real_name.strip(), 1)\n",
    "        updated_content.append(line)\n",
    "    \n",
    "    # Create a new file path with .FINAL before the extension\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    new_file_path = f\"{base}.FINAL{ext}\"\n",
    "    \n",
    "    # Write the updated content to the new file\n",
    "    with open(new_file_path, 'w') as file:\n",
    "        file.writelines(updated_content)\n",
    "\n",
    "    return new_file_path\n",
    "\n",
    "# Usage:\n",
    "new_file_path = update_speakers_with_real_names(file_path)\n",
    "print(f\"Updated file saved as: {new_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
