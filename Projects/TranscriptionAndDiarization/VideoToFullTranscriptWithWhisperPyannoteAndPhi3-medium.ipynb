{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook does the entire process\n",
    "\n",
    "It uses:\n",
    "- [pytube](https://pytube.io/en/latest/index.html)\n",
    "- [moviepy](https://pypi.org/project/moviepy/)\n",
    "- [transformers](https://github.com/huggingface/transformers)\n",
    "- [pyannote](https://pypi.org/project/pyannote.audio/)\n",
    "- [onnx-directml](https://pypi.org/project/onnxruntime-genai-directml/)\n",
    "\n",
    "You will need to download and install microsoft/Phi-3-medium-4k-instruct-onnx-directml and update the model_path below. If you do not have GPU or are not using Windows, see the Phi-3 docs and set yourself up accordingly.\n",
    "\n",
    "Pyannote.audio is gated on huggingface and requires an account and access key. See https://huggingface.co/pyannote/speaker-diarization-3.1 for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytube\n",
    "%pip install moviepy\n",
    "%pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate\n",
    "%pip install --upgrade pyannote.audio\n",
    "%pip install numpy\n",
    "%pip install --pre onnxruntime-genai-directml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfkey = \"YOUR HUGGINGFACE KEY HERE\"\n",
    "model_path = f'./directml/Phi-3-medium-4k-instruct'  # In tests I used medium-4k and medium-128k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and load openai/whisper-large-v3, pyannote/speaker-diarization-3.1, and Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyannote.audio import Pipeline\n",
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from pytube import YouTube\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import onnxruntime_genai as og\n",
    "import time\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=hfkey)\n",
    "\n",
    "diarization_pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define search options\n",
    "search_options = {\n",
    "    'do_sample': True,\n",
    "    'max_length': 3578,\n",
    "    'top_p': 0.9,\n",
    "    'top_k': 5,\n",
    "    'temperature': 0.5,\n",
    "    'repetition_penalty': 1.2\n",
    "}\n",
    "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = og.Model(model_path)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all the necessary functions\n",
    "\n",
    "There are some extras here for processing entire folders instead of individual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_video(video_url, output_path='./video'):\n",
    "    yt = YouTube(video_url)\n",
    "    # Get the highest resolution stream available\n",
    "    stream = yt.streams.get_highest_resolution()\n",
    "    # Download the video to the specified folder\n",
    "    stream.download(output_path) \n",
    "\n",
    "def extract_audio(video_file, output_audio_file):\n",
    "    video = VideoFileClip(video_file)\n",
    "    audio = video.audio\n",
    "    # Ensure the audio is in mono by setting nchannels to 1\n",
    "    audio.write_audiofile(output_audio_file, codec='mp3', ffmpeg_params=[\"-ac\", \"1\"])\n",
    "    audio.close()\n",
    "    video.close()\n",
    "\n",
    "def process_video_folder(input_folder, output_folder):\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".mp4\"):\n",
    "            video_path = os.path.join(input_folder, file)\n",
    "            audio_path = os.path.join(output_folder, os.path.splitext(file)[0] + '.mp3')\n",
    "            extract_audio(video_path, audio_path)\n",
    "            print(f\"Processed {file}\")\n",
    "\n",
    "def transcribe_and_save(audio_file_path, output_folder):\n",
    "    # Process the audio file using the pipe function\n",
    "    result = pipe(audio_file_path, return_timestamps=True)\n",
    "    # Extract the base name without the extension and create new file names\n",
    "    base_name = os.path.basename(os.path.splitext(audio_file_path)[0])\n",
    "    text_file_name = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "    json_file_name = os.path.join(output_folder, f\"{base_name}.json\")\n",
    "\n",
    "    # Write the transcription to a text file with timestamps\n",
    "    with open(text_file_name, 'w', encoding='utf-8') as text_file:\n",
    "        for chunk in result[\"chunks\"]:\n",
    "            # Formatting the timestamp\n",
    "            start, end = chunk['timestamp']\n",
    "            timestamp = f\"({start}, {end})\"\n",
    "            text_file.write(f\"[{timestamp}]: {chunk['text']}\\n\")\n",
    "\n",
    "    # Save the transcription as a JSON file\n",
    "    with open(json_file_name, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(result[\"chunks\"], json_file, indent=4)\n",
    "\n",
    "    print(f\"Transcription saved to {text_file_name}\")\n",
    "    print(f\"Transcription JSON saved to {json_file_name}\")\n",
    "\n",
    "def process_audio_folder(input_folder, output_folder):\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".mp3\"):\n",
    "            audio_file_path = os.path.join(input_folder, file)\n",
    "            transcribe_and_save(audio_file_path, output_folder)\n",
    "            print(f\"Processed {file}\")\n",
    "\n",
    "def diarize_audio(file_path, output_folder):\n",
    "    # Check if the file name contains spaces and handle it\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    if ' ' in base_name:\n",
    "        # Create a temporary file name by removing spaces\n",
    "        temp_file_path = os.path.join(os.path.dirname(file_path), base_name.replace(' ', '') + os.path.splitext(file_path)[1])\n",
    "        # Copy the original file to the new file with spaces removed\n",
    "        shutil.copyfile(file_path, temp_file_path)\n",
    "        # Use the new file path for processing\n",
    "        file_path = temp_file_path\n",
    "    else:\n",
    "        temp_file_path = None\n",
    "\n",
    "    # Perform diarization using the provided file path\n",
    "    diarization = diarization_pipeline(file_path)\n",
    "    \n",
    "    # Generate the output filename based on the input file's original name\n",
    "    output_file = os.path.join(output_folder, f\"{base_name}.rttm\")\n",
    "    \n",
    "    # Write the diarization output to disk using RTTM format\n",
    "    with open(output_file, \"w\") as rttm:\n",
    "        diarization.write_rttm(rttm)\n",
    "\n",
    "    # Cleanup: if a temporary file was used, delete it\n",
    "    if temp_file_path:\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    print(f\"Processed {file_path}, output to {output_file}\")\n",
    "\n",
    "def diarize_folder(input_folder, output_folder):\n",
    "    # List all files in the given folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        # Check if the file is an MP3 file\n",
    "        if file_name.endswith(\".mp3\"):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            diarize_audio(file_path, output_folder)\n",
    "\n",
    "def read_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def read_rttm(rttm_file):\n",
    "    rttm_data = []\n",
    "    with open(rttm_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            rttm_data.append({\n",
    "                \"turn_onset\": float(parts[3]),\n",
    "                \"duration\": float(parts[4]),\n",
    "                \"speaker\": parts[7]\n",
    "            })\n",
    "    return rttm_data\n",
    "\n",
    "def seconds_to_hms(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02}:{m:02}:{s:06.3f}\"\n",
    "\n",
    "def match_speaker(json_start, json_end, rttm_data):\n",
    "    best_match = None\n",
    "    max_overlap = -1\n",
    "    \n",
    "    for entry in rttm_data:\n",
    "        rttm_start = entry[\"turn_onset\"]\n",
    "        rttm_end = rttm_start + entry[\"duration\"]\n",
    "\n",
    "        overlap_start = max(json_start, rttm_start)\n",
    "        overlap_end = min(json_end, rttm_end)\n",
    "        overlap_duration = max(0, overlap_end - overlap_start)\n",
    "        \n",
    "        if overlap_duration > max_overlap:\n",
    "            max_overlap = overlap_duration\n",
    "            best_match = entry[\"speaker\"]\n",
    "\n",
    "    return best_match if best_match else \"Unknown\"\n",
    "\n",
    "def process_rttm_file(json_file, rttm_file, output_file):\n",
    "    json_data = read_json(json_file)\n",
    "    rttm_data = read_rttm(rttm_file)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as text_file:\n",
    "        previous_end = None  # Track the end time of the previous item\n",
    "        for i, chunk in enumerate(json_data):\n",
    "            start = chunk['timestamp'][0]\n",
    "            if len(chunk['timestamp']) > 1:\n",
    "                end = chunk['timestamp'][1]\n",
    "            \n",
    "            if end == None:\n",
    "                if i == len(json_data) - 1 and previous_end is not None:  # Last item with no end time\n",
    "                    end = previous_end + 5  # Assuming a reasonable default duration\n",
    "                elif i == len(json_data) - 1 and previous_end is None:  # Single item list\n",
    "                    end = start + 5  # Default duration for single item lists\n",
    "                else:\n",
    "                    continue  # If not the last item and no end time, skip\n",
    "            \n",
    "            previous_end = end  # Update the previous end time\n",
    "            speaker = match_speaker(start, end, rttm_data)\n",
    "            formatted_timestamp = f\"({seconds_to_hms(start)}, {seconds_to_hms(end)})\"\n",
    "            text_file.write(f\"[{speaker}] : [{formatted_timestamp}] : {chunk['text']}\\n\")\n",
    "\n",
    "def process_rttm_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            json_file = os.path.join(folder_path, file_name)\n",
    "            rttm_file = os.path.join(folder_path, base_name + '.rttm')\n",
    "            output_file = os.path.join(folder_path, base_name + '.transcript.txt')\n",
    "            \n",
    "            if os.path.exists(rttm_file):\n",
    "                process_rttm_file(json_file, rttm_file, output_file)\n",
    "                print(f\"Processed {json_file} and {rttm_file} into {output_file}\")\n",
    "            else:\n",
    "                print(f\"Warning: No RTTM file found for {json_file}. Skipping.\")\n",
    "\n",
    "def list_unique_speakers(file_path):\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find all unique speaker identifiers\n",
    "    speakers = re.findall(r\"\\[SPEAKER_(\\d{2})\\]\", content)\n",
    "    unique_speakers = sorted(set(speakers))\n",
    "    \n",
    "    # Format speakers in the form \"SPEAKER_XX\"\n",
    "    formatted_speakers = [f\"SPEAKER_{speaker}\" for speaker in unique_speakers]\n",
    "    return formatted_speakers\n",
    "\n",
    "def get_context_around_speaker(file_path, speaker):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Find the first occurrence of the speaker\n",
    "    for i, line in enumerate(lines):\n",
    "        if speaker in line:\n",
    "            start_index = max(0, i - 5)  # Ensure start index is within bounds\n",
    "            end_index = min(len(lines), i + 5)  # Ensure end index is within bounds (i+6 for inclusive slicing)\n",
    "            return lines[start_index:end_index]\n",
    "    return []  # Return an empty list if the speaker does not appear\n",
    "\n",
    "def collect_speaker_contexts(file_path):\n",
    "    # First, extract unique speakers from the file\n",
    "    unique_speakers = list_unique_speakers(file_path)\n",
    "    \n",
    "    # Now, use the second function to get context for each speaker\n",
    "    speaker_contexts = {}\n",
    "    for speaker in unique_speakers:\n",
    "        # Using the function to fetch context around each speaker's first mention\n",
    "        context = get_context_around_speaker(file_path, speaker)\n",
    "        speaker_contexts[speaker] = context\n",
    "    \n",
    "    return speaker_contexts     \n",
    "\n",
    "def process_llm_message(message, search_options):\n",
    "    # Prepare the prompt and encode it to tokens\n",
    "    prompt = chat_template.format(input=message)\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "    # Initialize generator parameters and the generator\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(**search_options)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "\n",
    "    # Initialize output message\n",
    "    output_message = ''\n",
    "\n",
    "    # Generate response\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        next_output = tokenizer.decode(new_token)\n",
    "        output_message += next_output\n",
    "\n",
    "    # Free up resources\n",
    "    del generator\n",
    "\n",
    "    return output_message\n",
    "\n",
    "def identify_speakers_with_llm(file_path):\n",
    "    \n",
    "    search_options = {\n",
    "        'do_sample': True,\n",
    "        'max_length': 3578,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 5,\n",
    "        'temperature': 0.5,\n",
    "        'repetition_penalty': 1.2\n",
    "    }\n",
    "\n",
    "    # First, gather the context for each speaker using the previously defined function\n",
    "    speaker_contexts = collect_speaker_contexts(file_path)\n",
    "    \n",
    "    # Iterate through each speaker and their associated context\n",
    "    responses = {}\n",
    "    for speaker, context in speaker_contexts.items():\n",
    "        # Join the context lines into a single string\n",
    "        context_text = ''.join(context).strip()\n",
    "        \n",
    "        # Create the prompt with the current speaker and their context\n",
    "        prompt = f\"\"\"\n",
    "        The following text is a piece of a transcript. Examine the text and if the text identifies the speaker reply with ONLY the full name matching the exact spelling of the person listed as {speaker} and nothing else. \n",
    "        ----------------------------------------\n",
    "        {context_text}\"\"\"\n",
    "\n",
    "        # Call the existing process_message function with the formatted prompt\n",
    "        response = process_llm_message(prompt, search_options)\n",
    "        \n",
    "        # Store the response for this speaker\n",
    "        responses[speaker] = response\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def update_speakers_with_real_names(file_path):\n",
    "    # Assume responses are in the form of speaker names we want to replace in the original file\n",
    "    response_dict = identify_speakers_with_llm(file_path)\n",
    "    \n",
    "    # Read the original file content\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "    \n",
    "    # Replace speaker labels with actual names based on response_dict\n",
    "    updated_content = []\n",
    "    for line in content:\n",
    "        for speaker, real_name in response_dict.items():\n",
    "            if speaker in line:\n",
    "                # Replace the first occurrence of speaker in the line with the real name\n",
    "                line = line.replace(speaker, real_name.strip(), 1)\n",
    "        updated_content.append(line)\n",
    "    \n",
    "    # Create a new file path with .FINAL before the extension\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    new_file_path = f\"{base}.FINAL{ext}\"\n",
    "    \n",
    "    # Write the updated content to the new file\n",
    "    with open(new_file_path, 'w') as file:\n",
    "        file.writelines(updated_content)\n",
    "\n",
    "    return new_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download a video\n",
    "download_video('https://www.youtube.com/watch?v=MI9DHkyH8Yk','./video')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./audio/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Next, extract the audio\n",
    "extract_audio('./video/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.mp4', './audio/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use whisper-large-v3 to create a transcript as text and as json\n",
    "\n",
    "The text is for reference, the json is used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use whisper-large to create a transcript\n",
    "transcribe_and_save('./audio/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.mp3', \"./transcript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use pyannote to diarize the audio and create an RTTM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ./audio\\Developer’sGuidetoCustomizingMicrosoftCopilot-MicrososftBuild2024.mp3, output to ./transcript\\Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.rttm\n"
     ]
    }
   ],
   "source": [
    "# Now use pyannote to diarize the audio\n",
    "diarize_audio('./audio/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.mp3', \"./transcript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the JSON and RTTM files to create a new transcript with speakers identified as SPEAKER_00, SPEAKER_01, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the files\n",
    "process_rttm_file('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.json', './Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.rttm', './Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, look for names using Phi-3 and create the 'FINAL' transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.FINAL.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the speakers with real names\n",
    "update_speakers_with_real_names('./Transcript/Developer’s Guide to Customizing Microsoft Copilot - Micrososft Build 2024.transcript.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
